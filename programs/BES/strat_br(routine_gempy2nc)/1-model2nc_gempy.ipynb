{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotina criada para converter o resultado do GemPy em NC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'osgeo'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gempy as gp\n",
    "import xarray as xr\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui determinamos alguns nomes de variáveis e parâmetros que serão utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"StratBR2GemPy_100x_100y_100z_2024-03-14-10-33-54.pkl\"\n",
    "model_n = os.path.splitext(model)[0]\n",
    "path_model = \"../../../output/BES/StartBR/novos_testes/gempy_2.3.1/\"\n",
    "fn_results = model_n + \"_results\"\n",
    "path_output = os.path.join(path_model, fn_results)\n",
    "\n",
    "# Cria o diretório de saida se não existir\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load no modelo\n",
    "geo_model = gp.load_model_pickle(path_model + model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos começar a extrair as infos do modelo gerado. \n",
    "\n",
    "Primeiro começamos com os surface points, orientation points, series e surfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "surfpoints = copy.copy(geo_model.surface_points.df)\n",
    "orientations = copy.copy(geo_model.orientations.df)\n",
    "surfaces = copy.copy(geo_model.surfaces.df)\n",
    "surfaces = surfaces.drop(columns=[\"vertices\", \"edges\"])\n",
    "series = copy.copy(geo_model.series.df)\n",
    "series.insert(1, \"series\", series.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esses dados vão ser salvos em .csv porque o tipo de dado das colunas são diferentes. Por iss, não podem ser adicionados no NC diretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria a pasta se não existir\n",
    "csv_results_path = os.path.join(path_output, \"csv_results\")\n",
    "if not os.path.exists(csv_results_path):\n",
    "    os.makedirs(csv_results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sfp = os.path.join(csv_results_path, \"surface_points.csv\")\n",
    "surfpoints.to_csv(path_sfp, index=False)\n",
    "\n",
    "path_ori = os.path.join(csv_results_path, \"orientations.csv\")\n",
    "orientations.to_csv(path_ori, index=False)\n",
    "\n",
    "path_series = os.path.join(csv_results_path, \"series.csv\")\n",
    "series.to_csv(path_series, index=False)\n",
    "\n",
    "path_surf = os.path.join(csv_results_path, \"surfaces.csv\")\n",
    "surfaces.to_csv(path_surf, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos começar a extrair mais dados adicionais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = geo_model.additional_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kriging_data_k = copy.copy(ad.kriging_data.df)\n",
    "kriging_data_k.insert(0, \"Model_ID\", model_n)\n",
    "path_kriging = os.path.join(csv_results_path, \"kriging_parameters.csv\")\n",
    "kriging_data_k.to_csv(path_kriging, index=False, mode=\"a\", header=not os.path.exists(path_kriging))\n",
    "\n",
    "rescaling_data_k = copy.copy(ad.rescaling_data.df)\n",
    "rescaling_data_k.insert(0, \"Model_ID\", model_n)\n",
    "path_rescale = os.path.join(csv_results_path, \"rescaling_parameters.csv\")\n",
    "rescaling_data_k.to_csv(path_rescale, index=False, mode=\"a\", header=not os.path.exists(path_rescale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos extrair o grid regular\n",
    "\n",
    "resolution, spacing e extent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames_rg = [\"x\", \"y\", \"z\"]\n",
    "\n",
    "resolution = geo_model.grid.regular_grid.resolution.reshape(1, -1)\n",
    "resolution_df = pd.DataFrame(data=resolution, columns=colnames_rg)\n",
    "path_res = os.path.join(csv_results_path, \"resolution.csv\")\n",
    "resolution_df.to_csv(path_res, index=False, mode=\"a\", header=not os.path.exists(path_res))\n",
    "\n",
    "spacing = np.array(geo_model.grid.regular_grid.get_dx_dy_dz()).reshape(1, -1)\n",
    "spacing_df = pd.DataFrame(data=spacing, columns=colnames_rg)\n",
    "path_space = os.path.join(csv_results_path, \"spacing.csv\")\n",
    "spacing_df.to_csv(path_space, index=False, mode=\"a\", header=not os.path.exists(path_space))\n",
    "\n",
    "extent = geo_model.grid.regular_grid.extent.reshape(1, -1)\n",
    "extent_df = pd.DataFrame(data=extent, columns=[\"xmin\", \"xmax\", \"ymin\", \"ymax\", \"zmin\", \"zmax\"])\n",
    "path_extent = os.path.join(csv_results_path, \"extent.csv\")\n",
    "extent_df.to_csv(path_extent, index=False, mode=\"a\", header=not os.path.exists(path_extent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com essas informações, acessadas, começamos a criação do NC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair X,Y,Z \n",
    "z_rg = geo_model.grid.regular_grid.z[::-1] # inverte o eixo z\n",
    "x_rg = geo_model.grid.regular_grid.x\n",
    "y_rg = geo_model.grid.regular_grid.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a dimensão\n",
    "nx = x_rg.size\n",
    "ny = y_rg.size\n",
    "nz = z_rg.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando coordenadas\n",
    "coords = {\"Model_ID\": model_n, \"nx\": x_rg, \"ny\": y_rg, \"nz\": z_rg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset(coords=coords)\n",
    "\n",
    "# Add as variaveis ao ds\n",
    "ds[\"lon\"] = (\"nx\", x_rg)\n",
    "ds[\"lat\"] = (\"ny\", y_rg)\n",
    "ds[\"depth\"] = (\"nz\", z_rg)\n",
    "\n",
    "# Add os atributos das variaveis\n",
    "ds[\"lon\"].attrs = {\n",
    "    \"long_name\": \"posição espacial longitudinal dos voxels\",\n",
    "    \"unit\": \"metro\",\n",
    "    \"var_desc\": \"CRS is EPSG:\",\n",
    "}\n",
    "\n",
    "ds[\"lat\"].attrs = {\n",
    "    \"long_name\": \"posição espacial latitudinal dos voxels\",\n",
    "    \"unit\": \"metro\",\n",
    "    \"var_desc\": \"CRS is EPSG:\",\n",
    "}\n",
    "\n",
    "ds[\"depth\"].attrs = {\n",
    "    \"long_name\": \"depth dos voxels\",\n",
    "    \"unit\": \"metro\",\n",
    "    \"var_desc\": \"depth é dada à unidade abaixo do nível do mar\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos extrair a solução do modelo:\n",
    "\n",
    "lith, scalar field, scalar field matrix, block matrix, mask matrix, mask matrix pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegamos os pontos do grid \n",
    "points_rg = geo_model.solutions.grid.get_grid(\"regular\")\n",
    "\n",
    "# -1 na serie e surfaces, porque a último é o basement\n",
    "n_series_active = series.index.size - 1\n",
    "n_surfaces_active = surfaces.index.size - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraimos o lith e iniciamos um array do tamanho da resolution\n",
    "lith_block_points = geo_model.solutions.lith_block\n",
    "lith_block_k = np.full((resolution[0][0], resolution[0][1], resolution[0][2]), np.nan)\n",
    "\n",
    "# Extraimos o scalar_field_matrix e iniciamos um array do tamanho da resolution e pro n número de séries\n",
    "scalar_matrix_points = geo_model.solutions.scalar_field_matrix\n",
    "scalar_field_matrix_k = np.full((n_series_active, resolution[0][0], resolution[0][1], resolution[0][2]), np.nan)\n",
    "\n",
    "# Extraimos o block_matrix e iniciamos um array do tamanho da resolution e pro n número de séries\n",
    "block_matrix_points = geo_model.solutions.block_matrix\n",
    "block_matrix_k = np.full((n_series_active, resolution[0][0], resolution[0][1], resolution[0][2]), np.nan)\n",
    "\n",
    "# Extraimos o mask_matrix e iniciamos um array do tamanho da resolution e pro n número de séries\n",
    "mask_matrix_points = geo_model.solutions.mask_matrix\n",
    "mask_matrix_k = np.full((n_series_active, resolution[0][0], resolution[0][1], resolution[0][2]), np.nan)\n",
    "\n",
    "# Extraimos o mask_matrix_pad\n",
    "mask_matrix_pad_k = np.array(geo_model.solutions.mask_matrix_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, para adicionar os valores nos seus respectivos arrays, vamos fazer um loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (x, y, z) in enumerate(points_rg):\n",
    "    is_x = x == x_rg\n",
    "    is_y = y == y_rg\n",
    "    is_z = z == z_rg\n",
    "\n",
    "    lith_block_k[is_x, is_y, is_z] = lith_block_points[idx]\n",
    "\n",
    "    for i in range(min(n_surfaces_active, n_series_active)):\n",
    "        scalar_field_matrix_k[i, is_x, is_y, is_z] = scalar_matrix_points[i, idx]\n",
    "        block_matrix_k[i, is_x, is_y, is_z] = block_matrix_points[i, 0, idx]\n",
    "        mask_matrix_k[i, is_x, is_y, is_z] = mask_matrix_points[i, idx]\n",
    "\n",
    "lith_block_k = np.swapaxes(lith_block_k, 0, 2)\n",
    "scalar_field_matrix_k = np.swapaxes(scalar_field_matrix_k, 1, 3)\n",
    "block_matrix_k = np.swapaxes(block_matrix_k, 1, 3)\n",
    "mask_matrix_k = np.swapaxes(mask_matrix_k, 1, 3)\n",
    "mask_matrix_pad_k = np.swapaxes(mask_matrix_pad_k, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E agora pegamos o valor do scalar field das surface points\n",
    "scalar_field_surfpoints = geo_model.solutions.scalar_field_at_surface_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora fazemos a adição desses arrays no NC com seus respectivos atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"lith_block\"] = ((\"nz\", \"ny\", \"nx\"), lith_block_k)\n",
    "ds[\"scalar_field_matrix\"] = ((\"n_active_series\", \"nz\", \"ny\", \"nx\"), scalar_field_matrix_k)\n",
    "ds[\"block_matrix\"] = ((\"n_active_series\", \"nz\", \"ny\", \"nx\"), block_matrix_k)\n",
    "ds[\"mask_matrix\"] = ((\"n_active_series\", \"nz\", \"ny\", \"nx\"), mask_matrix_k)\n",
    "ds[\"mask_matrix_pad\"] = ((\"n_active_series\", \"nz\", \"ny\", \"nx\"), mask_matrix_pad_k)\n",
    "ds[\"scalar_field_at_surface_points\"] = ((\"n_active_series\", \"n_active_surfaces\"), scalar_field_surfpoints)\n",
    "\n",
    "# Add attributes to the variables\n",
    "ds[\"lith_block\"].attrs = {\n",
    "    \"long_name\": \"ID das superficies definidas\",\n",
    "    \"unit\": \"-\",\n",
    "    \"var_desc\": \"São floats, porém o ID é um valor inteiro (int)\",\n",
    "}\n",
    "\n",
    "ds[\"scalar_field_matrix\"].attrs = {\n",
    "    \"long_name\": \"Mariz com o scalar field\",\n",
    "    \"unit\": \"-\",\n",
    "    \"var_desc\": \"Valores de scalar field para cada localização do regular grid\",\n",
    "}\n",
    "\n",
    "ds[\"block_matrix\"].attrs = {\n",
    "    \"long_name\": \"Matriz contendo os valores de ID interpolados\",\n",
    "    \"unit\": \"-\",\n",
    "    \"var_desc\": \"Matriz com todos os valores interpolados para todas as series, em cada localização do regular grid\",\n",
    "}\n",
    "\n",
    "ds[\"mask_matrix\"].attrs = {\n",
    "    \"long_name\": \"Matriz booleana contendo infos para as combinações das series\",\n",
    "    \"unit\": \"-\",\n",
    "    \"var_desc\": \"Contém a lógica para combinar as series para obter o modelo final em cada localização do regular grid\",\n",
    "}\n",
    "\n",
    "ds[\"mask_matrix_pad\"].attrs = {\n",
    "    \"long_name\": \"Matriz booleana contendo infos para as combinações das séries\",\n",
    "    \"unit\": \"-\",\n",
    "    \"var_desc\": \"Mask preenchida com blocos 2x2 pra garantir a interseção das camadas após marching cubes (algoritmo de isosuperficie)\",\n",
    "}\n",
    "\n",
    "ds[\"scalar_field_at_surface_points\"].attrs = {\n",
    "    \"long_name\": \"Valor do scalar field em cada interface\",\n",
    "    \"unit\": \"-\",\n",
    "    \"var_desc\": \"O eixo 0 é cada série e o eixo 1 é cada superficie ordenada por seu id\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(os.path.join(path_output, f\"{model_n}.nc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos extrair o resultado dos verticies e edges pra cada surface e salvar em .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a pasta, se não existir\n",
    "path_surf = os.path.join(path_output, \"triangulated_surfaces\")\n",
    "if not os.path.isdir(path_surf):\n",
    "    os.mkdir(path_surf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando os arrays\n",
    "vertices = [np.empty((0, 3), dtype=float)] * len(geo_model.solutions.vertices)\n",
    "edges = [np.empty((0, 3), dtype=int)] * len(geo_model.solutions.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_k = geo_model.solutions.vertices\n",
    "edges_k = geo_model.solutions.edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazemos um loop nas verticies para acessar cada uma das surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(vertices)):\n",
    "    # Anexa a loc espacial dos pontos \n",
    "    # A exceção ocorre quando a surface não está presente\n",
    "    try:\n",
    "        vertices[idx] = np.append(vertices[idx], vertices_k[idx], axis=0)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    # Encontra o valor máximo do edge e anexa\n",
    "    try:\n",
    "        max_edge = np.max(edges[idx]) if not edges[idx].size == 0 else 0\n",
    "        edges_k[idx] += int(max_edge)\n",
    "        edges[idx] = np.append(edges[idx], edges_k[idx], axis=0)\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done model id StratBR2GemPy_100x_100y_100z_2024-03-14-10-33-54, ../../../output/BES/StartBR/novos_testes/gempy_2.3.1/\n"
     ]
    }
   ],
   "source": [
    "# Acessa cada surface e salva as verticies e edges em csv\n",
    "for idx, (vv, ee) in enumerate(zip(vertices, edges)):\n",
    "    vert = pd.DataFrame(data=vv, columns=[\"x\", \"y\", \"z\"])\n",
    "    path_vert = os.path.join(path_surf, \"vertices_id-\" + str(idx) + \".csv\")\n",
    "    vert.to_csv(path_vert, index=False, mode=\"a\", header=not os.path.exists(path_vert))\n",
    "\n",
    "    edge = pd.DataFrame(data=ee, columns=[\"idx1\", \"idx2\", \"idx3\"])\n",
    "    path_edge = os.path.join(path_surf, \"edges_id-\" + str(idx) + \".csv\")\n",
    "    edge.to_csv(path_edge, index=False, mode=\"a\", header=not os.path.exists(path_edge))\n",
    "\n",
    "print(f\"Done model id {model_n}, {path_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemgis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
